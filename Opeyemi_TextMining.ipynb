{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHejWTCXjNsW"
      },
      "source": [
        "**Opeyemi Adeniran**<br>\n",
        "**COSC 611**<br>\n",
        "**Final Project- Text Mining Analysis of Movie Dataset**<br>\n",
        "**December 4, 2021**<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuSB8cD1PVIM"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfMBEF8gPV2C"
      },
      "source": [
        "!wget -q https://apache.osuosl.org/spark/spark-3.1.3/spark-3.1.3-bin-hadoop2.7.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYaovCBcPWEp"
      },
      "source": [
        "!tar xf spark-3.1.3-bin-hadoop2.7.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q24T1bUPWKQ"
      },
      "source": [
        "pip install -q findspark"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aUoP8F6PWNJ"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.3-bin-hadoop2.7\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6IYu6zDqPWPp",
        "outputId": "ca39e020-fb36-4d2b-f6cc-126a76178cc1"
      },
      "source": [
        "os.environ[\"SPARK_HOME\"]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.1.3-bin-hadoop2.7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5u6GLDhPWSA",
        "outputId": "b92f5e83-fccc-4a16-f7b0-f2a3f7d8b6a0"
      },
      "source": [
        "pip install spark-nlp==2.4.2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spark-nlp==2.4.2\n",
            "  Downloading spark_nlp-2.4.2-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE76oEEBPWUZ",
        "outputId": "898bc00b-853e-4e49-deff-f299aac7d28a"
      },
      "source": [
        "pip install pyspark==3.1.3"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark==3.1.3 in /usr/local/lib/python3.8/dist-packages (3.1.3)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.8/dist-packages (from pyspark==3.1.3) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHfTWUKVPWWx",
        "outputId": "ad655502-e343-4c16-e671-c3907664ce0f"
      },
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  2.4.2\n",
            "Apache Spark version:  3.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSkN9p29zT3W",
        "outputId": "a7a8ce21-6c77-4d95-e3f3-367b3ea3449a"
      },
      "source": [
        "pip install langid"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from langid) (1.21.6)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941188 sha256=3d547ebb021c036a7698e5f6394f7d0ac1b9358ea6e67bde5a253d38b600baa5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/01/a4/0160c55074707b535a6757a541842817d530d8080ca943a107\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbAAKg9p1diL",
        "outputId": "cfef6603-83fb-43f7-d5b6-6e16e2ec7384"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcYbAGzTqLcG",
        "outputId": "b057185d-329f-49bb-e881-6ec9c07f912a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AzxF--9SJ9p"
      },
      "source": [
        "import string"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mTpk822ecCD"
      },
      "source": [
        "# 14.2. Text Preprocessing\n",
        "# NOTE: Rerun from here if you get SparkContexts error. \n",
        "def check_blanks(data_str):\n",
        "    is_blank = str(data_str.isspace())\n",
        "    return is_blank"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Ix6RkNV3vj"
      },
      "source": [
        "# Remove features\n",
        "import re\n",
        "import string\n",
        "def remove_features(data_str):\n",
        "    # compile regex\n",
        "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
        "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    punc_re = re.compile('[^A-Za-z0-9]+') \n",
        "    num_re = re.compile('(\\\\d+)')\n",
        "    mention_re = re.compile('@(\\w+)')\n",
        "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
        "    # convert to lowercase\n",
        "    data_str = data_str.lower()\n",
        "    # remove hyperlinks\n",
        "    data_str = url_re.sub(' ', data_str)\n",
        "    # remove @mentions\n",
        "    data_str = mention_re.sub(' ', data_str)\n",
        "    # remove puncuation\n",
        "    data_str = punc_re.sub(' ', data_str)\n",
        "    # remove numeric 'words'\n",
        "    data_str = num_re.sub(' ', data_str)\n",
        "    # remove non a-z 0-9 characters and words shorter than 3 characters\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    for word in data_str.split():\n",
        "        if list_pos == 0:\n",
        "            if alpha_num_re.match(word) and len(word) > 2:\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = ' '\n",
        "        else:\n",
        "            if alpha_num_re.match(word) and len(word) > 2:\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            else:\n",
        "                cleaned_str += ' '\n",
        "        list_pos += 1\n",
        "    return cleaned_str"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qkrr-83iC4c",
        "outputId": "e14b0904-83b3-4b66-dd80-2c2dc76970cf"
      },
      "source": [
        "# Removes stop words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords.words('english')\n",
        "print(stopwords.words('english'))\n",
        "#print(stops)\n",
        "def remove_stops(data_str):\n",
        "    # expects a string\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    text = data_str.split()\n",
        "    for word in text:\n",
        "        if word not in stopwords:\n",
        "            # rebuild cleaned_str\n",
        "            if list_pos == 0:\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            list_pos += 1\n",
        "    return cleaned_str"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k6EX89hiGS7",
        "outputId": "89e8e40e-0208-4cc5-fbf5-bd4b0899e3cf"
      },
      "source": [
        "# Tagging text\n",
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def tag_and_remove(data_str):\n",
        "    cleaned_str = ' '\n",
        "    # noun tags\n",
        "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
        "    # adjectives\n",
        "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
        "    # verbs\n",
        "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
        "\n",
        "    # break string into 'words'\n",
        "    text = data_str.split()\n",
        "\n",
        "    # tag the text and keep only those with the right tags\n",
        "    tagged_text = nltk.pos_tag(text)\n",
        "    for tagged_word in tagged_text:\n",
        "        if tagged_word[1] in nltk_tags:\n",
        "            cleaned_str += tagged_word[0] + ' '\n",
        "\n",
        "    return cleaned_str"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoxXnDTMiILq",
        "outputId": "2b841060-af9c-439d-b9f7-0cad9668a847"
      },
      "source": [
        "# Lemmatization \n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "def lemmatize(data_str):\n",
        "    # expects a string\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = data_str.split()\n",
        "    tagged_words = nltk.pos_tag(text)\n",
        "    for word in tagged_words:\n",
        "        if 'v' in word[1].lower():\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
        "        else:\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
        "        if list_pos == 0:\n",
        "            cleaned_str = lemma\n",
        "        else:\n",
        "            cleaned_str = cleaned_str + ' ' + lemma\n",
        "        list_pos += 1\n",
        "    return cleaned_str"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvIYygH6W9NN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8c9ca515-fcbe-4c57-c1d9-d911a0687d9b"
      },
      "source": [
        "'''\n",
        "# Functions provided for preprocessing - Please ignore\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "#import preproc as pp\n",
        "check_lang_udf = udf(check_lang, StringType())\n",
        "remove_stops_udf = udf(remove_stops, StringType())\n",
        "remove_features_udf = udf(remove_features, StringType())\n",
        "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
        "lemmatize_udf = udf(lemmatize, StringType())\n",
        "check_blanks_udf = udf(check_blanks, StringType())\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Functions provided for preprocessing - Please ignore\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import StringType\\n#import preproc as pp\\ncheck_lang_udf = udf(check_lang, StringType())\\nremove_stops_udf = udf(remove_stops, StringType())\\nremove_features_udf = udf(remove_features, StringType())\\ntag_and_remove_udf = udf(tag_and_remove, StringType())\\nlemmatize_udf = udf(lemmatize, StringType())\\ncheck_blanks_udf = udf(check_blanks, StringType())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vh_LX_vlvfy",
        "outputId": "03fcf1e4-ac4d-4604-f5f7-8465dec00a29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbz-OoFAY1em",
        "outputId": "446a7ef3-866f-4904-f51c-60bf1458827e"
      },
      "source": [
        "# Execution for loading dataset\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "#import preproc as pp\n",
        "'''\n",
        "check_lang_udf = udf(check_lang, StringType())\n",
        "'''\n",
        "remove_stops_udf = udf(remove_stops, StringType())\n",
        "remove_features_udf = udf(remove_features, StringType())\n",
        "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
        "lemmatize_udf = udf(lemmatize, StringType())\n",
        "check_blanks_udf = udf(check_blanks, StringType())\n",
        "\n",
        "# create Spark context with Spark configuration\n",
        "\n",
        "sc = pyspark.SparkContext()\n",
        "sqlContext = SQLContext(sc)\n",
        "'''\n",
        "# import\n",
        "from pyspark.sql import HiveContext\n",
        "sc = pyspark.SparkContext()\n",
        "sqlContext = HiveContext(sc)\n",
        "#spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n",
        "'''\n",
        "# Uncomment here\n",
        "# Read file into RDD\n",
        "\n",
        "neg_rdd = sc.textFile(\"/content/drive/MyDrive/FINAL PROJECT/POLORITY/all_text/neg.txt\")\n",
        "pos_rdd = sc.textFile(\"/content/drive/MyDrive/FINAL PROJECT/POLORITY/all_text/pos.txt\")\n",
        "\n",
        "neg_parts_rdd = neg_rdd.map(lambda l: l.split(\"\\t\"))\n",
        "pos_parts_rdd = neg_rdd.map(lambda l: l.split(\"\\t\"))\n",
        "# Filter bad rows out\n",
        "neg_filled_rdd = neg_parts_rdd.filter(bool)\n",
        "pos_filled_rdd = pos_parts_rdd.filter(bool)\n",
        "#typed_rdd = data_rdd.map(lambda p: (p[0], p[1], float(p[2])))\n",
        "\n",
        "#Create DataFrame\n",
        "neg_data_df = sqlContext.createDataFrame(neg_filled_rdd, StringType())\n",
        "pos_data_df = sqlContext.createDataFrame(pos_filled_rdd, StringType())\n",
        "# get the raw columns\n",
        "neg_raw_cols = neg_data_df.columns\n",
        "pos_raw_cols = pos_data_df.columns\n",
        "#raw_cols = lines.columns\n",
        "\n",
        "neg_data_df.printSchema()\n",
        "pos_data_df.printSchema()\n",
        "#lines.printSchema()\n",
        "neg_data_df.na.drop(how=\"any\").show(truncate=False)\n",
        "pos_data_df.na.drop(how=\"any\").show(truncate=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                                         |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[in the past , tim burton has taken cinema by storm with the action packed batman , and the hilarious ed wood , but lately his films just don't cut it . ]                    |\n",
            "|[sleepy hollow has some unique features , but the script is a terrible mess . ]                                                                                               |\n",
            "|[for those who remember disney's fantastic \" adventures of ichabod crane \" , it is actually superior to this . ]                                                              |\n",
            "|[disney's version was at least interesting , while burton's film drags on far too long , without any suspense or frights . ]                                                  |\n",
            "|[while the stories are similar , the new version adds too much to the once likable story , and throws in some weak dialogue to top it off . ]                                 |\n",
            "|[ichabod crane ( johnny depp ) is now apparently a constable , who was sent to new york to investigate suspicious murders . ]                                                 |\n",
            "|[the victims are headless , and no traces of evidence were found , until the murderer reveals himself , hence the headless horseman . ]                                       |\n",
            "|[now with the help of christina ricci and an orphaned boy , they must stop the headless horseman from a killing spree , that could destroy the whole town of sleepy hollow . ]|\n",
            "|[sure the headless horseman is an intriguing character , but the story has so many holes that even he nor depp can save this disaster . ]                                     |\n",
            "|[in the original , the headless horseman was meant to be unstoppable . ]                                                                                                      |\n",
            "|[no one could stop him , not even ichabod . ]                                                                                                                                 |\n",
            "|[by the end of the film , it had audiences in deep thought . ]                                                                                                                |\n",
            "|[burton however concentrates far too much on the atmosphere , and how to make the main characters heroes . ]                                                                  |\n",
            "|[he takes no risk what so ever , and this is where he fails . ]                                                                                                               |\n",
            "|[another pathetic element of the film was the script . ]                                                                                                                      |\n",
            "|[like i said earlier , he just stuffed too much in at one time , and made it look real sloppy , kind of like the mess the headless horseman made with his victims . ]         |\n",
            "|[he completely butchered the story to pieces . ]                                                                                                                              |\n",
            "|[thankfully there are a few factors that i particularly liked . ]                                                                                                             |\n",
            "|[the acting by depp was quite notable . ]                                                                                                                                     |\n",
            "|[he took the nerdy character of the cartoon , and simply acted it out in a better manner . ]                                                                                  |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                                         |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[in the past , tim burton has taken cinema by storm with the action packed batman , and the hilarious ed wood , but lately his films just don't cut it . ]                    |\n",
            "|[sleepy hollow has some unique features , but the script is a terrible mess . ]                                                                                               |\n",
            "|[for those who remember disney's fantastic \" adventures of ichabod crane \" , it is actually superior to this . ]                                                              |\n",
            "|[disney's version was at least interesting , while burton's film drags on far too long , without any suspense or frights . ]                                                  |\n",
            "|[while the stories are similar , the new version adds too much to the once likable story , and throws in some weak dialogue to top it off . ]                                 |\n",
            "|[ichabod crane ( johnny depp ) is now apparently a constable , who was sent to new york to investigate suspicious murders . ]                                                 |\n",
            "|[the victims are headless , and no traces of evidence were found , until the murderer reveals himself , hence the headless horseman . ]                                       |\n",
            "|[now with the help of christina ricci and an orphaned boy , they must stop the headless horseman from a killing spree , that could destroy the whole town of sleepy hollow . ]|\n",
            "|[sure the headless horseman is an intriguing character , but the story has so many holes that even he nor depp can save this disaster . ]                                     |\n",
            "|[in the original , the headless horseman was meant to be unstoppable . ]                                                                                                      |\n",
            "|[no one could stop him , not even ichabod . ]                                                                                                                                 |\n",
            "|[by the end of the film , it had audiences in deep thought . ]                                                                                                                |\n",
            "|[burton however concentrates far too much on the atmosphere , and how to make the main characters heroes . ]                                                                  |\n",
            "|[he takes no risk what so ever , and this is where he fails . ]                                                                                                               |\n",
            "|[another pathetic element of the film was the script . ]                                                                                                                      |\n",
            "|[like i said earlier , he just stuffed too much in at one time , and made it look real sloppy , kind of like the mess the headless horseman made with his victims . ]         |\n",
            "|[he completely butchered the story to pieces . ]                                                                                                                              |\n",
            "|[thankfully there are a few factors that i particularly liked . ]                                                                                                             |\n",
            "|[the acting by depp was quite notable . ]                                                                                                                                     |\n",
            "|[he took the nerdy character of the cartoon , and simply acted it out in a better manner . ]                                                                                  |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDwGYYNqz0bL",
        "outputId": "2bbb2d94-cf29-4f19-c3b2-e578005aae9e"
      },
      "source": [
        "# Remove stopwords from values\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords.words('english')\n",
        "neg_rm_stops_df = neg_data_df.select(neg_raw_cols).withColumn(\"neg_stop_text\", remove_stops_udf(neg_data_df[\"value\"]))\n",
        "neg_rm_stops_df.show(20)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|               value|       neg_stop_text|\n",
            "+--------------------+--------------------+\n",
            "|[in the past , ti...|[in past , tim bu...|\n",
            "|[sleepy hollow ha...|[sleepy hollow un...|\n",
            "|[for those who re...|[for remember dis...|\n",
            "|[disney's version...|[disney's version...|\n",
            "|[while the storie...|[while stories si...|\n",
            "|[ichabod crane ( ...|[ichabod crane ( ...|\n",
            "|[the victims are ...|[the victims head...|\n",
            "|[now with the hel...|[now help christi...|\n",
            "|[sure the headles...|[sure headless ho...|\n",
            "|[in the original ...|[in original , he...|\n",
            "|[no one could sto...|[no one could sto...|\n",
            "|[by the end of th...|[by end film , au...|\n",
            "|[burton however c...|[burton however c...|\n",
            "|[he takes no risk...|[he takes risk ev...|\n",
            "|[another pathetic...|[another pathetic...|\n",
            "|[like i said earl...|[like said earlie...|\n",
            "|[he completely bu...|[he completely bu...|\n",
            "|[thankfully there...|[thankfully facto...|\n",
            "|[the acting by de...|[the acting depp ...|\n",
            "|[he took the nerd...|[he took nerdy ch...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebee6ac-6697-4130-9b26-c4318e4a934a",
        "id": "xGfYbg5Hs3eO"
      },
      "source": [
        "# Remove stopwords from values\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords.words('english')\n",
        "pos_rm_stops_df = pos_data_df.select(pos_raw_cols).withColumn(\"pos_stop_text\", remove_stops_udf(pos_data_df[\"value\"]))\n",
        "pos_rm_stops_df.show(20)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|               value|       pos_stop_text|\n",
            "+--------------------+--------------------+\n",
            "|[in the past , ti...|[in past , tim bu...|\n",
            "|[sleepy hollow ha...|[sleepy hollow un...|\n",
            "|[for those who re...|[for remember dis...|\n",
            "|[disney's version...|[disney's version...|\n",
            "|[while the storie...|[while stories si...|\n",
            "|[ichabod crane ( ...|[ichabod crane ( ...|\n",
            "|[the victims are ...|[the victims head...|\n",
            "|[now with the hel...|[now help christi...|\n",
            "|[sure the headles...|[sure headless ho...|\n",
            "|[in the original ...|[in original , he...|\n",
            "|[no one could sto...|[no one could sto...|\n",
            "|[by the end of th...|[by end film , au...|\n",
            "|[burton however c...|[burton however c...|\n",
            "|[he takes no risk...|[he takes risk ev...|\n",
            "|[another pathetic...|[another pathetic...|\n",
            "|[like i said earl...|[like said earlie...|\n",
            "|[he completely bu...|[he completely bu...|\n",
            "|[thankfully there...|[thankfully facto...|\n",
            "|[the acting by de...|[the acting depp ...|\n",
            "|[he took the nerd...|[he took nerdy ch...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJellzSBIu4U",
        "outputId": "6c1a12a8-39a9-402b-b12e-6abd76e9507a"
      },
      "source": [
        "# Remove features from values\n",
        "import re\n",
        "import string\n",
        "neg_rm_features_df = neg_rm_stops_df.select(neg_raw_cols+[\"neg_stop_text\"]).withColumn(\"neg_feat_text\", remove_features_udf(neg_rm_stops_df[\"neg_stop_text\"]))\n",
        "neg_rm_features_df.show(20)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|       neg_stop_text|       neg_feat_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[in the past , ti...|[in past , tim bu...|  past tim burton...|\n",
            "|[sleepy hollow ha...|[sleepy hollow un...|sleepy hollow uni...|\n",
            "|[for those who re...|[for remember dis...|for remember disn...|\n",
            "|[disney's version...|[disney's version...|disney  version l...|\n",
            "|[while the storie...|[while stories si...|while stories sim...|\n",
            "|[ichabod crane ( ...|[ichabod crane ( ...|ichabod crane joh...|\n",
            "|[the victims are ...|[the victims head...|the victims headl...|\n",
            "|[now with the hel...|[now help christi...|now help christin...|\n",
            "|[sure the headles...|[sure headless ho...|sure headless hor...|\n",
            "|[in the original ...|[in original , he...|  original headle...|\n",
            "|[no one could sto...|[no one could sto...|  one could stop ...|\n",
            "|[by the end of th...|[by end film , au...|  end film audien...|\n",
            "|[burton however c...|[burton however c...|burton however co...|\n",
            "|[he takes no risk...|[he takes risk ev...|  takes risk ever...|\n",
            "|[another pathetic...|[another pathetic...|another pathetic ...|\n",
            "|[like i said earl...|[like said earlie...|like said earlier...|\n",
            "|[he completely bu...|[he completely bu...|  completely butc...|\n",
            "|[thankfully there...|[thankfully facto...|thankfully factor...|\n",
            "|[the acting by de...|[the acting depp ...|the acting depp q...|\n",
            "|[he took the nerd...|[he took nerdy ch...|  took nerdy char...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0eelFozuLxQ",
        "outputId": "303fff1b-cdd8-4b69-ae1d-be7ddb795613"
      },
      "source": [
        "# Remove features from values\n",
        "import re\n",
        "import string\n",
        "pos_rm_features_df = pos_rm_stops_df.select(pos_raw_cols+[\"pos_stop_text\"]).withColumn(\"pos_feat_text\", remove_features_udf(pos_rm_stops_df[\"pos_stop_text\"]))\n",
        "pos_rm_features_df.show(20)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|       pos_stop_text|       pos_feat_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[in the past , ti...|[in past , tim bu...|  past tim burton...|\n",
            "|[sleepy hollow ha...|[sleepy hollow un...|sleepy hollow uni...|\n",
            "|[for those who re...|[for remember dis...|for remember disn...|\n",
            "|[disney's version...|[disney's version...|disney  version l...|\n",
            "|[while the storie...|[while stories si...|while stories sim...|\n",
            "|[ichabod crane ( ...|[ichabod crane ( ...|ichabod crane joh...|\n",
            "|[the victims are ...|[the victims head...|the victims headl...|\n",
            "|[now with the hel...|[now help christi...|now help christin...|\n",
            "|[sure the headles...|[sure headless ho...|sure headless hor...|\n",
            "|[in the original ...|[in original , he...|  original headle...|\n",
            "|[no one could sto...|[no one could sto...|  one could stop ...|\n",
            "|[by the end of th...|[by end film , au...|  end film audien...|\n",
            "|[burton however c...|[burton however c...|burton however co...|\n",
            "|[he takes no risk...|[he takes risk ev...|  takes risk ever...|\n",
            "|[another pathetic...|[another pathetic...|another pathetic ...|\n",
            "|[like i said earl...|[like said earlie...|like said earlier...|\n",
            "|[he completely bu...|[he completely bu...|  completely butc...|\n",
            "|[thankfully there...|[thankfully facto...|thankfully factor...|\n",
            "|[the acting by de...|[the acting depp ...|the acting depp q...|\n",
            "|[he took the nerd...|[he took nerdy ch...|  took nerdy char...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3JDtFLfuutb",
        "outputId": "2bf7e74c-0a5e-4ed1-8376-3a4b92f170f7"
      },
      "source": [
        "# Tagging text from value\n",
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "neg_tagged_df = neg_rm_features_df.select(neg_raw_cols+[\"neg_feat_text\"]).withColumn(\"neg_tagged_text\", tag_and_remove_udf(neg_rm_features_df.neg_feat_text))\n",
        "neg_tagged_df.show(20)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|       neg_feat_text|     neg_tagged_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[in the past , ti...|  past tim burton...| past tim burton ...|\n",
            "|[sleepy hollow ha...|sleepy hollow uni...| sleepy hollow un...|\n",
            "|[for those who re...|for remember disn...| remember disney ...|\n",
            "|[disney's version...|disney  version l...| disney version l...|\n",
            "|[while the storie...|while stories sim...| stories similar ...|\n",
            "|[ichabod crane ( ...|ichabod crane joh...| ichabod crane jo...|\n",
            "|[the victims are ...|the victims headl...| victims headless...|\n",
            "|[now with the hel...|now help christin...| help christina r...|\n",
            "|[sure the headles...|sure headless hor...| sure headless ho...|\n",
            "|[in the original ...|  original headle...| original headles...|\n",
            "|[no one could sto...|  one could stop ...|       stop ichabod |\n",
            "|[by the end of th...|  end film audien...| end film audienc...|\n",
            "|[burton however c...|burton however co...| burton concentra...|\n",
            "|[he takes no risk...|  takes risk ever...|   takes risk fails |\n",
            "|[another pathetic...|another pathetic ...| pathetic element...|\n",
            "|[like i said earl...|like said earlier...| said much time m...|\n",
            "|[he completely bu...|  completely butc...| butchered story ...|\n",
            "|[thankfully there...|thankfully factor...|      factors liked |\n",
            "|[the acting by de...|the acting depp q...| acting depp nota...|\n",
            "|[he took the nerd...|  took nerdy char...| took nerdy chara...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeMEY_zBGFin",
        "outputId": "098280f2-e26e-4e45-dad4-0414e3db1a33"
      },
      "source": [
        "# Tagging text from value\n",
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_tagged_df = pos_rm_features_df.select(pos_raw_cols+[\"pos_feat_text\"]).withColumn(\"pos_tagged_text\", tag_and_remove_udf(pos_rm_features_df.pos_feat_text))\n",
        "pos_tagged_df.show(20)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|       pos_feat_text|     pos_tagged_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[in the past , ti...|  past tim burton...| past tim burton ...|\n",
            "|[sleepy hollow ha...|sleepy hollow uni...| sleepy hollow un...|\n",
            "|[for those who re...|for remember disn...| remember disney ...|\n",
            "|[disney's version...|disney  version l...| disney version l...|\n",
            "|[while the storie...|while stories sim...| stories similar ...|\n",
            "|[ichabod crane ( ...|ichabod crane joh...| ichabod crane jo...|\n",
            "|[the victims are ...|the victims headl...| victims headless...|\n",
            "|[now with the hel...|now help christin...| help christina r...|\n",
            "|[sure the headles...|sure headless hor...| sure headless ho...|\n",
            "|[in the original ...|  original headle...| original headles...|\n",
            "|[no one could sto...|  one could stop ...|       stop ichabod |\n",
            "|[by the end of th...|  end film audien...| end film audienc...|\n",
            "|[burton however c...|burton however co...| burton concentra...|\n",
            "|[he takes no risk...|  takes risk ever...|   takes risk fails |\n",
            "|[another pathetic...|another pathetic ...| pathetic element...|\n",
            "|[like i said earl...|like said earlier...| said much time m...|\n",
            "|[he completely bu...|  completely butc...| butchered story ...|\n",
            "|[thankfully there...|thankfully factor...|      factors liked |\n",
            "|[the acting by de...|the acting depp q...| acting depp nota...|\n",
            "|[he took the nerd...|  took nerdy char...| took nerdy chara...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX9RYWxIQKM3",
        "outputId": "9f265903-0be1-4ad9-90fc-0ed0abdd3c06"
      },
      "source": [
        "# Lemmatization for values\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "neg_lemm_df = neg_tagged_df.select(neg_raw_cols+[\"neg_tagged_text\"]).withColumn(\"neg_lemm_text\", lemmatize_udf(neg_tagged_df[\"neg_tagged_text\"]))\n",
        "neg_lemm_df.show(20)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|     neg_tagged_text|       neg_lemm_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[in the past , ti...| past tim burton ...|past tim burton t...|\n",
            "|[sleepy hollow ha...| sleepy hollow un...|sleepy hollow uni...|\n",
            "|[for those who re...| remember disney ...|remember disney f...|\n",
            "|[disney's version...| disney version l...|disney version le...|\n",
            "|[while the storie...| stories similar ...|story similar new...|\n",
            "|[ichabod crane ( ...| ichabod crane jo...|ichabod crane joh...|\n",
            "|[the victims are ...| victims headless...|victim headless t...|\n",
            "|[now with the hel...| help christina r...|help christina ri...|\n",
            "|[sure the headles...| sure headless ho...|sure headless hor...|\n",
            "|[in the original ...| original headles...|original headless...|\n",
            "|[no one could sto...|       stop ichabod |        stop ichabod|\n",
            "|[by the end of th...| end film audienc...|end film audience...|\n",
            "|[burton however c...| burton concentra...|burton concentrat...|\n",
            "|[he takes no risk...|   takes risk fails |     take risk fails|\n",
            "|[another pathetic...| pathetic element...|pathetic element ...|\n",
            "|[like i said earl...| said much time m...|say much time mak...|\n",
            "|[he completely bu...| butchered story ...| butcher story piece|\n",
            "|[thankfully there...|      factors liked |         factor like|\n",
            "|[the acting by de...| acting depp nota...|    act depp notable|\n",
            "|[he took the nerd...| took nerdy chara...|take nerdy charac...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44bfa01-60f9-40e1-d2d4-97d55bb80bb7",
        "id": "OmkjqfcRvgzZ"
      },
      "source": [
        "# Lemmatization for values\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "pos_lemm_df = pos_tagged_df.select(pos_raw_cols+[\"pos_tagged_text\"]).withColumn(\"pos_lemm_text\", lemmatize_udf(pos_tagged_df[\"pos_tagged_text\"]))\n",
        "pos_lemm_df.show(20)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               value|     pos_tagged_text|       pos_lemm_text|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[in the past , ti...| past tim burton ...|past tim burton t...|\n",
            "|[sleepy hollow ha...| sleepy hollow un...|sleepy hollow uni...|\n",
            "|[for those who re...| remember disney ...|remember disney f...|\n",
            "|[disney's version...| disney version l...|disney version le...|\n",
            "|[while the storie...| stories similar ...|story similar new...|\n",
            "|[ichabod crane ( ...| ichabod crane jo...|ichabod crane joh...|\n",
            "|[the victims are ...| victims headless...|victim headless t...|\n",
            "|[now with the hel...| help christina r...|help christina ri...|\n",
            "|[sure the headles...| sure headless ho...|sure headless hor...|\n",
            "|[in the original ...| original headles...|original headless...|\n",
            "|[no one could sto...|       stop ichabod |        stop ichabod|\n",
            "|[by the end of th...| end film audienc...|end film audience...|\n",
            "|[burton however c...| burton concentra...|burton concentrat...|\n",
            "|[he takes no risk...|   takes risk fails |     take risk fails|\n",
            "|[another pathetic...| pathetic element...|pathetic element ...|\n",
            "|[like i said earl...| said much time m...|say much time mak...|\n",
            "|[he completely bu...| butchered story ...| butcher story piece|\n",
            "|[thankfully there...|      factors liked |         factor like|\n",
            "|[the acting by de...| acting depp nota...|    act depp notable|\n",
            "|[he took the nerd...| took nerdy chara...|take nerdy charac...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqjLxLRcRkGO",
        "outputId": "2985855d-5eb6-42a8-8d48-6963d67de252"
      },
      "source": [
        "neg_check_blanks_df = neg_lemm_df.select(neg_raw_cols+[\"neg_lemm_text\"]).withColumn(\"is_blank\", check_blanks_udf(neg_lemm_df[\"neg_lemm_text\"]))\n",
        "# remove blanks from lemmatized text\n",
        "neg_no_blanks_df = neg_check_blanks_df.filter(neg_check_blanks_df[\"is_blank\"] == \"False\")\n",
        "\n",
        "# drop duplicates from lemmatized text\n",
        "neg_dedup_df = neg_no_blanks_df.dropDuplicates(['neg_lemm_text'])\n",
        "\n",
        "neg_dedup_df.show(20)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------+\n",
            "|               value|       neg_lemm_text|is_blank|\n",
            "+--------------------+--------------------+--------+\n",
            "|[the acting is pr...|act bad jackie ch...|   False|\n",
            "|[some action film...|action film actio...|   False|\n",
            "|[the action scene...|action scene shoo...|   False|\n",
            "|[now that he has ...|adoptive son burt...|   False|\n",
            "|[the aforemention...|aforementioned cl...|   False|\n",
            "|[alas betty has t...|ala dubious disti...|   False|\n",
            "|[apparently , thi...|alien lifeform su...|   False|\n",
            "|[is there anythin...|       anything good|   False|\n",
            "|[and is there any...|anything spectacu...|   False|\n",
            "|[apart from these...|apart factor noth...|   False|\n",
            "|[the army should ...|army save time br...|   False|\n",
            "|      [art ! \" ) , ]|                 art|   False|\n",
            "|[astaire and roge...|      astaire rogers|   False|\n",
            "|[any attempt at a...|attempt anything ...|   False|\n",
            "|[plus , the attem...|attempt make char...|   False|\n",
            "|[its not that bad...|bad movie great h...|   False|\n",
            "|[unfortunately , ...|bat prime example...|   False|\n",
            "|[the best part of...|best part film lo...|   False|\n",
            "|[the bottom line ...|bottom line bad m...|   False|\n",
            "|[and that the \" b...|brain cloud thing...|   False|\n",
            "+--------------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMzHtJX_yLms",
        "outputId": "9dbfe961-94e8-4ba5-fd0e-5378a37be563"
      },
      "source": [
        "pos_check_blanks_df = pos_lemm_df.select(pos_raw_cols+[\"pos_lemm_text\"]).withColumn(\"is_blank\", check_blanks_udf(pos_lemm_df[\"pos_lemm_text\"]))\n",
        "# remove blanks from lemmatized text\n",
        "pos_no_blanks_df = pos_check_blanks_df.filter(pos_check_blanks_df[\"is_blank\"] == \"False\")\n",
        "\n",
        "# drop duplicates from lemmatized text\n",
        "pos_dedup_df = pos_no_blanks_df.dropDuplicates(['pos_lemm_text'])\n",
        "\n",
        "pos_dedup_df.show(20)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------+\n",
            "|               value|       pos_lemm_text|is_blank|\n",
            "+--------------------+--------------------+--------+\n",
            "|[the acting is pr...|act bad jackie ch...|   False|\n",
            "|[some action film...|action film actio...|   False|\n",
            "|[the action scene...|action scene shoo...|   False|\n",
            "|[now that he has ...|adoptive son burt...|   False|\n",
            "|[the aforemention...|aforementioned cl...|   False|\n",
            "|[alas betty has t...|ala dubious disti...|   False|\n",
            "|[apparently , thi...|alien lifeform su...|   False|\n",
            "|[is there anythin...|       anything good|   False|\n",
            "|[and is there any...|anything spectacu...|   False|\n",
            "|[apart from these...|apart factor noth...|   False|\n",
            "|[the army should ...|army save time br...|   False|\n",
            "|      [art ! \" ) , ]|                 art|   False|\n",
            "|[astaire and roge...|      astaire rogers|   False|\n",
            "|[any attempt at a...|attempt anything ...|   False|\n",
            "|[plus , the attem...|attempt make char...|   False|\n",
            "|[its not that bad...|bad movie great h...|   False|\n",
            "|[unfortunately , ...|bat prime example...|   False|\n",
            "|[the best part of...|best part film lo...|   False|\n",
            "|[the bottom line ...|bottom line bad m...|   False|\n",
            "|[and that the \" b...|brain cloud thing...|   False|\n",
            "+--------------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmkd0xHt7TdE",
        "outputId": "085da6a1-4394-4e3f-e24c-1b60a51b3aca"
      },
      "source": [
        "# Rename final column as text for processing\n",
        "pos_final_df = pos_dedup_df.selectExpr(\"pos_lemm_text as text\")\n",
        "pos_final_df.show()\n",
        "pos_final_df.printSchema()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|film rat director...|\n",
            "|   movie originality|\n",
            "|       anything good|\n",
            "|             letdown|\n",
            "|think joblo get l...|\n",
            "|anything spectacu...|\n",
            "|attempt make char...|\n",
            "|yank us water pis...|\n",
            "|let begin say wor...|\n",
            "|fill brim wham ba...|\n",
            "|hand controversia...|\n",
            "|highlight movie a...|\n",
            "|imgen company fun...|\n",
            "|performance under...|\n",
            "|fall hop joel sch...|\n",
            "|kelsey grammer go...|\n",
            "|response big lecture|\n",
            "|try catch stupid ...|\n",
            "|show bad as want ...|\n",
            "|                 art|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ik1p98Ey-5Y",
        "outputId": "93d1654e-77f8-4103-a870-e5d14768de56"
      },
      "source": [
        "# Rename final column as text for processing\n",
        "neg_final_df = neg_dedup_df.selectExpr(\"neg_lemm_text as text\")\n",
        "neg_final_df.show()\n",
        "neg_final_df.printSchema()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|film rat director...|\n",
            "|   movie originality|\n",
            "|       anything good|\n",
            "|             letdown|\n",
            "|think joblo get l...|\n",
            "|anything spectacu...|\n",
            "|attempt make char...|\n",
            "|yank us water pis...|\n",
            "|let begin say wor...|\n",
            "|fill brim wham ba...|\n",
            "|hand controversia...|\n",
            "|highlight movie a...|\n",
            "|imgen company fun...|\n",
            "|performance under...|\n",
            "|fall hop joel sch...|\n",
            "|kelsey grammer go...|\n",
            "|response big lecture|\n",
            "|try catch stupid ...|\n",
            "|show bad as want ...|\n",
            "|                 art|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM7E-uXZSHs3",
        "outputId": "ce21a43c-00fd-47ef-d940-ac0b5c2abc7b"
      },
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "# Create Unique ID for text\n",
        "neg_uid_df = neg_final_df.withColumn(\"neg_uid\", monotonically_increasing_id())\n",
        "neg_uid_df.show(4)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+\n",
            "|                text|neg_uid|\n",
            "+--------------------+-------+\n",
            "|film rat director...|      0|\n",
            "|   movie originality|      1|\n",
            "|       anything good|      2|\n",
            "|             letdown|      3|\n",
            "+--------------------+-------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wRnoUdXzZVR",
        "outputId": "a01da2ac-ae18-438a-a6ce-3b44a7154d6d"
      },
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "# Create Unique ID for text\n",
        "pos_uid_df = pos_final_df.withColumn(\"pos_uid\", monotonically_increasing_id())\n",
        "pos_uid_df.show(4)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+\n",
            "|                text|pos_uid|\n",
            "+--------------------+-------+\n",
            "|film rat director...|      0|\n",
            "|   movie originality|      1|\n",
            "|       anything good|      2|\n",
            "|             letdown|      3|\n",
            "+--------------------+-------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbEqaJOg87He",
        "outputId": "983fbcc6-2327-419f-a700-fa56c77f6121"
      },
      "source": [
        "# Create label with 1.0 being the constant for ML classifier\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(\"double\")\n",
        "def const_col():\n",
        "    return 1.0\n",
        "\n",
        "neg_label_df = neg_uid_df.withColumn('label', const_col())\n",
        "neg_label_df.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+-----+\n",
            "|                text|neg_uid|label|\n",
            "+--------------------+-------+-----+\n",
            "|film rat director...|      0|  1.0|\n",
            "|   movie originality|      1|  1.0|\n",
            "|       anything good|      2|  1.0|\n",
            "|             letdown|      3|  1.0|\n",
            "|think joblo get l...|      4|  1.0|\n",
            "|anything spectacu...|      5|  1.0|\n",
            "|attempt make char...|      6|  1.0|\n",
            "|yank us water pis...|      7|  1.0|\n",
            "|let begin say wor...|      8|  1.0|\n",
            "|fill brim wham ba...|      9|  1.0|\n",
            "|hand controversia...|     10|  1.0|\n",
            "|highlight movie a...|     11|  1.0|\n",
            "|imgen company fun...|     12|  1.0|\n",
            "|performance under...|     13|  1.0|\n",
            "|fall hop joel sch...|     14|  1.0|\n",
            "|kelsey grammer go...|     15|  1.0|\n",
            "|response big lecture|     16|  1.0|\n",
            "|try catch stupid ...|     17|  1.0|\n",
            "|show bad as want ...|     18|  1.0|\n",
            "|                 art|     19|  1.0|\n",
            "+--------------------+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTmOwp420GhA",
        "outputId": "f3ab2ae8-fe50-449e-9e8e-e1ed9e81ea38"
      },
      "source": [
        "# Create label with 1.0 being the constant for ML classifier\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(\"double\")\n",
        "def const_col():\n",
        "    return 1.0\n",
        "\n",
        "pos_label_df = pos_uid_df.withColumn('label', const_col())\n",
        "pos_label_df.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+-----+\n",
            "|                text|pos_uid|label|\n",
            "+--------------------+-------+-----+\n",
            "|film rat director...|      0|  1.0|\n",
            "|   movie originality|      1|  1.0|\n",
            "|       anything good|      2|  1.0|\n",
            "|             letdown|      3|  1.0|\n",
            "|think joblo get l...|      4|  1.0|\n",
            "|anything spectacu...|      5|  1.0|\n",
            "|attempt make char...|      6|  1.0|\n",
            "|yank us water pis...|      7|  1.0|\n",
            "|let begin say wor...|      8|  1.0|\n",
            "|fill brim wham ba...|      9|  1.0|\n",
            "|hand controversia...|     10|  1.0|\n",
            "|highlight movie a...|     11|  1.0|\n",
            "|imgen company fun...|     12|  1.0|\n",
            "|performance under...|     13|  1.0|\n",
            "|fall hop joel sch...|     14|  1.0|\n",
            "|kelsey grammer go...|     15|  1.0|\n",
            "|response big lecture|     16|  1.0|\n",
            "|try catch stupid ...|     17|  1.0|\n",
            "|show bad as want ...|     18|  1.0|\n",
            "|                 art|     19|  1.0|\n",
            "+--------------------+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_FVwbzLSY7m",
        "outputId": "e9e2444d-3927-4a33-9f40-b9c1f84d106a"
      },
      "source": [
        "pos_data = pos_label_df.select('pos_uid','text','label')\n",
        "pos_data.show(20)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-----+\n",
            "|pos_uid|                text|label|\n",
            "+-------+--------------------+-----+\n",
            "|      0|film rat director...|  1.0|\n",
            "|      1|   movie originality|  1.0|\n",
            "|      2|       anything good|  1.0|\n",
            "|      3|             letdown|  1.0|\n",
            "|      4|think joblo get l...|  1.0|\n",
            "|      5|anything spectacu...|  1.0|\n",
            "|      6|attempt make char...|  1.0|\n",
            "|      7|yank us water pis...|  1.0|\n",
            "|      8|let begin say wor...|  1.0|\n",
            "|      9|fill brim wham ba...|  1.0|\n",
            "|     10|hand controversia...|  1.0|\n",
            "|     11|highlight movie a...|  1.0|\n",
            "|     12|imgen company fun...|  1.0|\n",
            "|     13|performance under...|  1.0|\n",
            "|     14|fall hop joel sch...|  1.0|\n",
            "|     15|kelsey grammer go...|  1.0|\n",
            "|     16|response big lecture|  1.0|\n",
            "|     17|try catch stupid ...|  1.0|\n",
            "|     18|show bad as want ...|  1.0|\n",
            "|     19|                 art|  1.0|\n",
            "+-------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI_fZ2SG0bMl",
        "outputId": "c1d2b64c-9cf2-4c5d-88c9-f655a59ffaa8"
      },
      "source": [
        "neg_data = neg_label_df.select('neg_uid','text','label')\n",
        "neg_data.show(20)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-----+\n",
            "|neg_uid|                text|label|\n",
            "+-------+--------------------+-----+\n",
            "|      0|film rat director...|  1.0|\n",
            "|      1|   movie originality|  1.0|\n",
            "|      2|       anything good|  1.0|\n",
            "|      3|             letdown|  1.0|\n",
            "|      4|think joblo get l...|  1.0|\n",
            "|      5|anything spectacu...|  1.0|\n",
            "|      6|attempt make char...|  1.0|\n",
            "|      7|yank us water pis...|  1.0|\n",
            "|      8|let begin say wor...|  1.0|\n",
            "|      9|fill brim wham ba...|  1.0|\n",
            "|     10|hand controversia...|  1.0|\n",
            "|     11|highlight movie a...|  1.0|\n",
            "|     12|imgen company fun...|  1.0|\n",
            "|     13|performance under...|  1.0|\n",
            "|     14|fall hop joel sch...|  1.0|\n",
            "|     15|kelsey grammer go...|  1.0|\n",
            "|     16|response big lecture|  1.0|\n",
            "|     17|try catch stupid ...|  1.0|\n",
            "|     18|show bad as want ...|  1.0|\n",
            "|     19|                 art|  1.0|\n",
            "+-------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y25NYlxGSiU3"
      },
      "source": [
        "# Split the data into training and test sets (40% held out for testing)\n",
        "(pos_trainingData, pos_testData) = pos_data.randomSplit([0.6, 0.4])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J70_3H9o03y_"
      },
      "source": [
        "# Split the data into training and test sets (40% held out for testing)\n",
        "(neg_trainingData, neg_testData) = neg_data.randomSplit([0.6, 0.4])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Naive Bayes classifier.\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, DecisionTreeClassifier\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "#data = tokenizer.transform(data)\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
        "labelEncoder = StringIndexer(inputCol=\"text\", outputCol='label')\n",
        "#vectorizer = CountVectorizer(inputCol= \"words\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(minDocFreq=2000, inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "#assembler = VectorAssembler(inputCols=[\"hour\", \"mobile\", \"userFeatures\"],outputCol=\"features\")\n",
        "#idfModel = idf.fit(data)\n",
        "\n",
        "#lda = LDA(k=20, seed=1, optimizer=\"em\")\n",
        "\n",
        "# DecisionTree Classifier\n",
        "#dt = DecisionTreeClassifier()\n",
        "\n",
        "# Linear Support Vector Classifer\n",
        "#lsvc = LinearSVC((maxIter=10, regParam=0.1)\n",
        "\n",
        "# Naive Bayes model\n",
        "nb = NaiveBayes(smoothing=2.0)\n",
        "\n",
        "# Random Forest Classifier\n",
        "#rfc = RandomForestClassifier()\n",
        "\n",
        "# Pipeline Architecture for NB\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
        "\n",
        "# Pipeline Architecture for RFC\n",
        "#pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, rfc])\n",
        "\n",
        "# Pipeline Architecture for DT\n",
        "#pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, dt])\n",
        "\n",
        "# Pipeline Architecture for LSVC\n",
        "#pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lsvc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StMRYnDVKJL5",
        "outputId": "81b089e8-f984-4611-e388-de4edf0954cf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PipelineModel_55a67adacadb"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model.  This also runs the indexers.\n",
        "neg_model = pipeline.fit(neg_trainingData)\n",
        "neg_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUzmSwJAeyFR",
        "outputId": "b975be62-5b60-442f-b6ff-30c7b9cce119"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PipelineModel_aa7dc5f8a8d0"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model.  This also runs the indexers.\n",
        "pos_model = pipeline.fit(pos_trainingData)\n",
        "pos_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm3I9zy2e8bN",
        "outputId": "f7deba5c-8163-4dd3-fe60-0ceaf1674f7f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PipelineModel_0f070dbda5e2"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2DlsYuZfPQp",
        "outputId": "dca65a0a-21ac-4bd0-dfec-856fddaf9c3f"
      },
      "source": [
        "pos_predictions = pos_model.transform(pos_testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "pos_predictions.select(\"text\", \"label\", \"prediction\").show(5,True)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+\n",
            "|                text|label|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|think joblo get l...|  1.0|       0.0|\n",
            "|anything spectacu...|  1.0|       0.0|\n",
            "|yank us water pis...|  1.0|       0.0|\n",
            "|fill brim wham ba...|  1.0|       0.0|\n",
            "|imgen company fun...|  1.0|       0.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "733c77b9-705d-41f9-cd57-ced8b9da51c4",
        "id": "r4h7JIBih-LG"
      },
      "source": [
        "neg_predictions = neg_model.transform(neg_testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "neg_predictions.select(\"text\", \"label\", \"prediction\").show(5,True)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+\n",
            "|                text|label|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|film rat director...|  1.0|       0.0|\n",
            "|       anything good|  1.0|       0.0|\n",
            "|anything spectacu...|  1.0|       0.0|\n",
            "|attempt make char...|  1.0|       0.0|\n",
            "|let begin say wor...|  1.0|       0.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTKAQRoRsZ3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76704869-943b-4b82-d3b3-eeb641d8d204"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "neg_evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
        "neg_evaluator.evaluate(neg_predictions)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8b51df-2190-4a1a-f3f9-06a3ff7f7690",
        "id": "kkmjxK6pkqTi"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "pos_evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
        "pos_evaluator.evaluate(pos_predictions)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9pvzLFwsO0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8150c8-b3bf-440e-d788-0ce07db4a961"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "pos_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "pos_evaluator.evaluate(pos_predictions)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlccfDu8k6fm",
        "outputId": "ed631637-eb04-4e04-bc21-bf33e8fb5e64"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "neg_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "neg_evaluator.evaluate(neg_predictions)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0yk6qGGoLj_",
        "outputId": "45f838bd-1b1e-4e99-91d9-8f89bfd93b4f"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "pos_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "pos_evaluator.evaluate(pos_predictions)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29UeX35QlgHE",
        "outputId": "c8c1c690-3f67-410d-f64a-b594d6735f55"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "neg_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "neg_evaluator.evaluate(neg_predictions)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVSiOLUaOobM"
      },
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "def sentiment_analysis(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "sentiment_analysis_udf = udf(sentiment_analysis , FloatType())"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PErj_fJ2OzZV",
        "outputId": "2151e028-8d8e-487d-a854-cfa938c4e2ca"
      },
      "source": [
        "df = neg_label_df.withColumn(\"sentiment_score\", sentiment_analysis_udf(neg_label_df['text'] ))\n",
        "df.show(20,True)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+-----+---------------+\n",
            "|                text|neg_uid|label|sentiment_score|\n",
            "+--------------------+-------+-----+---------------+\n",
            "|film rat director...|      0|  1.0|         0.3125|\n",
            "|   movie originality|      1|  1.0|            0.0|\n",
            "|       anything good|      2|  1.0|            0.7|\n",
            "|             letdown|      3|  1.0|            0.0|\n",
            "|think joblo get l...|      4|  1.0|    -0.14107142|\n",
            "|anything spectacu...|      5|  1.0|            0.6|\n",
            "|attempt make char...|      6|  1.0|           0.15|\n",
            "|yank us water pis...|      7|  1.0|            0.0|\n",
            "|let begin say wor...|      8|  1.0|           -0.5|\n",
            "|fill brim wham ba...|      9|  1.0|     0.25654763|\n",
            "|hand controversia...|     10|  1.0|     0.19166666|\n",
            "|highlight movie a...|     11|  1.0|            0.0|\n",
            "|imgen company fun...|     12|  1.0|     0.13636364|\n",
            "|performance under...|     13|  1.0|            0.0|\n",
            "|fall hop joel sch...|     14|  1.0|           0.45|\n",
            "|kelsey grammer go...|     15|  1.0|            0.7|\n",
            "|response big lecture|     16|  1.0|            0.0|\n",
            "|try catch stupid ...|     17|  1.0|       -0.36875|\n",
            "|show bad as want ...|     18|  1.0|         -0.375|\n",
            "|                 art|     19|  1.0|            0.0|\n",
            "+--------------------+-------+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBsJvrfsnzWj",
        "outputId": "b6474ffc-b953-4ac3-9d83-51d585882886"
      },
      "source": [
        "df2 = pos_label_df.withColumn(\"sentiment_score\", sentiment_analysis_udf(pos_label_df['text'] ))\n",
        "df2.show(20,True)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+-----+---------------+\n",
            "|                text|pos_uid|label|sentiment_score|\n",
            "+--------------------+-------+-----+---------------+\n",
            "|film rat director...|      0|  1.0|         0.3125|\n",
            "|   movie originality|      1|  1.0|            0.0|\n",
            "|       anything good|      2|  1.0|            0.7|\n",
            "|             letdown|      3|  1.0|            0.0|\n",
            "|think joblo get l...|      4|  1.0|    -0.14107142|\n",
            "|anything spectacu...|      5|  1.0|            0.6|\n",
            "|attempt make char...|      6|  1.0|           0.15|\n",
            "|yank us water pis...|      7|  1.0|            0.0|\n",
            "|let begin say wor...|      8|  1.0|           -0.5|\n",
            "|fill brim wham ba...|      9|  1.0|     0.25654763|\n",
            "|hand controversia...|     10|  1.0|     0.19166666|\n",
            "|highlight movie a...|     11|  1.0|            0.0|\n",
            "|imgen company fun...|     12|  1.0|     0.13636364|\n",
            "|performance under...|     13|  1.0|            0.0|\n",
            "|fall hop joel sch...|     14|  1.0|           0.45|\n",
            "|kelsey grammer go...|     15|  1.0|            0.7|\n",
            "|response big lecture|     16|  1.0|            0.0|\n",
            "|try catch stupid ...|     17|  1.0|       -0.36875|\n",
            "|show bad as want ...|     18|  1.0|         -0.375|\n",
            "|                 art|     19|  1.0|            0.0|\n",
            "+--------------------+-------+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LXdpO1dO45e"
      },
      "source": [
        "def condition(r):\n",
        "    if (r >=0.1):\n",
        "        label = \"positive\"\n",
        "    elif(r <= -0.1):\n",
        "        label = \"negative\"\n",
        "    else:\n",
        "        label = \"neutral\"\n",
        "    return label\n",
        "\n",
        "sentiment_udf = udf(lambda x: condition(x), StringType())"
      ],
      "execution_count": 82,
      "outputs": []
    }
  ]
}